import os
import json
from typing import TypedDict, Optional

from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage

from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, START, END


CHAT_MODEL = 'qwen3:8B'

class ChatState(TypedDict):
    messages: list
    uploaded_file_content: Optional[str]

app_state = {'messages': [], 'uploaded_file_content': None}

user_instructions = ""

@tool
def analyze_and_edit_file():
    """Analyzes the content of an already uploaded file and edits it based on user instructions."""
    global app_state, user_instructions

    if not app_state.get('uploaded_file_content'):
        return "No file content available for analysis. Please ensure content is provided."

    file_content = app_state['uploaded_file_content']

    prompt = f"""You are an expert code reviewer and editor. The following is the content of a Python file:
```python
{file_content}
```
User instructions for editing: {user_instructions}
Please provide the edited code. If no specific edit instructions are given, just review it and suggest improvements."""

    try:
        modified_code = raw_llm.invoke(prompt).content
        return f"```python\n{modified_code}\n```"
    except Exception as e:
        return f"Error during file analysis and editing: {e}"

@tool
def save_code_to_file(code_content: str, file_name: str):
    """
    Saves the provided code content to a file in the 'Outputs' directory.
    This tool should be used when the AI generates code and the user asks to save it.
    Args:
        code_content (str): The code to be saved. This should be the complete code generated by the AI.
        file_name (str): The name of the file (e.g., "my_script.py"). This argument is required.
    """
    output_directory = "Outputs"

    try:
        if not os.path.isdir(output_directory):
            os.makedirs(output_directory)

        file_path = os.path.join(output_directory, file_name)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(code_content)
        return f"Code successfully saved to {file_path}"
    except Exception as e:
        return f"Failed to save code to file: {e}"


def llm_node(state):
    system_message = SystemMessage(content="""You are a helpful AI assistant specialized in Python programming.
When the user asks you to write a Python script and explicitly asks to save it with a file name (e.g., "save it as my_script.py"),
you MUST follow these steps:
1. Generate the complete Python code.
2. Immediately after generating the code, call the 'save_code_to_file' tool.
   - The 'code_content' argument for the tool must be the full Python code you just generated.
   - The 'file_name' argument for the tool must be extracted directly from the user's request (e.g., "my_script.py").
   - Example of a tool call: `tool_code: save_code_to_file(code_content="print('Hello')", file_name="hello.py")`
The code will always be saved in the 'Outputs' directory. You do not need to ask the user for a directory path.
If the user asks for code but does not specify a file name, just provide the code directly in the chat.
""")
    messages_for_llm = [system_message] + state['messages']

    response = llm.invoke(messages_for_llm)

    return {'messages': state['messages'] + [response]}


def router(state):
    last_message = state['messages'][-1]

    tool_calls = getattr(last_message, 'tool_calls', [])
    if tool_calls and isinstance(tool_calls, list):
        return "tools"
    else:
        return 'end'


tool_node = ToolNode([analyze_and_edit_file, save_code_to_file])

def tools_node(state):
    result = tool_node.invoke(state)

    tool_output_messages = result.get('messages', [])
    new_messages = state['messages'] + tool_output_messages
    return {'messages': new_messages}


llm = init_chat_model(CHAT_MODEL, model_provider='ollama')
llm = llm.bind_tools([analyze_and_edit_file, save_code_to_file])

raw_llm = init_chat_model(CHAT_MODEL, model_provider='ollama')

builder = StateGraph(ChatState)
builder.add_node('llm', llm_node)
builder.add_node('tools', tools_node)
builder.add_edge(START, 'llm')
builder.add_edge('tools', 'llm')
builder.add_conditional_edges('llm', router, {'tools': 'tools', 'end': END})

graph = builder.compile()

def process_agent_request(user_message: str, uploaded_file_content: Optional[str] = None) -> str:
    """
    Processes a user request through the agent graph.
    This function acts as the entry point for a backend system.

    Args:
        user_message (str): The message from the user (e.g., "write python code for X").
        uploaded_file_content (Optional[str]): The content of a file, if one was "uploaded"
                                                by the backend system.

    Returns:
        str: The agent's final response or tool output.
    """
    global app_state, user_instructions

    app_state['messages'] = []
    app_state['uploaded_file_content'] = uploaded_file_content

    user_instructions = user_message

    app_state['messages'].append(HumanMessage(content=user_message))

    final_graph_output = None
    for s in graph.stream(app_state):
        if 'llm' in s:
            final_graph_output = s['llm']
        elif 'tools' in s:
            final_graph_output = s['tools']

        if final_graph_output and 'messages' in final_graph_output:
            app_state['messages'] = final_graph_output['messages']

    if 'messages' in app_state and app_state['messages']:
        last_message = app_state['messages'][-1]
        if isinstance(last_message, AIMessage):
            if last_message.content:
                return last_message.content
            elif last_message.tool_calls:
                return f"AI requested tool call: {last_message.tool_calls}. Processing..."
            else:
                return "AI finished processing without explicit content."
        elif isinstance(last_message, ToolMessage):
            return f"Tool Output: {last_message.content}"
        else:
            return f"Agent Response: {last_message}"
    else:
        return "No response from agent."


if __name__ == '__main__':
    print("Backend Agent Script - Simulating Requests")
    print("------------------------------------------")
    print("This script now acts as a backend. You can simulate requests.")
    print("Type 'quit' to exit.")
    print("\nExample 1: Generate and save code")
    print("Input: 'find nth fibonacci number and save it as fib.py'")
    print("Expected Output: Confirmation that fib.py is saved in Outputs/.")

    print("\nExample 2: Analyze and edit (requires prior 'upload' simulation)")
    print("Input: 'analyze this code and make it more efficient' (with simulated file content)")
    print("Expected Output: Analysis/edited code.")

    while True:
        simulated_user_input = input("\nSimulate user request (or 'quit'): ")

        if simulated_user_input.lower() == 'quit':
            break

        response = process_agent_request(simulated_user_input)
        print(f"\nAgent Response: {response}")

        if "analyze" in simulated_user_input.lower() and "file" in simulated_user_input.lower():
            sample_file_content = """
def inefficient_sum(arr):
    total = 0
    for i in range(len(arr)):
        for j in range(i + 1):
            total += arr[j]
    return total
"""
            print("\nSimulating file content for analysis...")
            response_after_upload_sim = process_agent_request(
                "Analyze the uploaded code and suggest efficiency improvements.",
                uploaded_file_content=sample_file_content
            )
            print(f"Agent Response after simulated upload and analysis request: {response_after_upload_sim}")
